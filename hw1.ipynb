{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irene951/ML/blob/main/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B5JCzsRtmMfc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.linalg import eigh\n",
        "from scipy.linalg import inv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG_6ohH5BqqQ"
      },
      "source": [
        "#**Load Datasets**\n",
        "\n",
        "### Overview of the Iris Dataset\n",
        "The dataset contains 150 samples of flowers from three different species of iris.\n",
        "\n",
        "Each species has 50 samples, and the dataset provides four features for each flower, measured in centimeters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tVqSQjhxmOe9"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-y9slnOBwHY"
      },
      "source": [
        "Part 1: Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is an **unsupervised** method that aims to find the directions (principal components) that maximize the **variance** of the data.\n",
        "\n",
        "## Implementation Steps:\n",
        "1. Standardize the data.\n",
        "2. Calculate the Covariance Matrix.\n",
        "3. Solve for Eigenvalues and Eigenvectors.\n",
        "4. Select the top $k$ components and project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvPcHpU8mUmm"
      },
      "outputs": [],
      "source": [
        "def standardize_data(X):\n",
        "    \"\"\"Standardize data (zero mean, unit variance)\"\"\"\n",
        "    mean = np.mean(X, axis=0)\n",
        "    std_dev = np.std(X, axis=0)\n",
        "    std_dev[std_dev == 0] = 1\n",
        "    X_std = (X - mean) / std_dev\n",
        "    return X_std\n",
        "\n",
        "def calculate_covariance_matrix(X_std):\n",
        "    \"\"\"Calculate the Covariance Matrix\"\"\"\n",
        "    n_samples = X_std.shape[0]\n",
        "    # TODO: Calculate the Covariance Matrix (Hint: Matrix multiplication)\n",
        "    cov_matrix = np.dot(X_std.T, X_std)/(n_samples-1)\n",
        "    return cov_matrix\n",
        "\n",
        "def pca(X, n_components):\n",
        "    \"\"\"Execute PCA\"\"\"\n",
        "    X_std = standardize_data(X)\n",
        "    cov_matrix = calculate_covariance_matrix(X_std)\n",
        "\n",
        "    # Solve for Eigenvalues/vectors\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "    eigen_pairs = [(eigenvalues[i], eigenvectors[:, i]) for i in range(len(eigenvalues))]\n",
        "\n",
        "    # Sort: descending order\n",
        "    # TODO: Fill in the key for sorting Eigenpairs\n",
        "    eigen_pairs.sort(key= # 【BLANK 2: Eigenvalue sorting key】 , reverse=True)\n",
        "\n",
        "    W = np.array([eigen_pairs[i][1] for i in range(n_components)]).T\n",
        "\n",
        "    # Project\n",
        "    # TODO: Fill in the projection formula\n",
        "    X_pca = # 【BLANK 3: Projection formula】\n",
        "\n",
        "    return X_pca, W, eigen_pairs\n",
        "\n",
        "# Execute PCA (Reduce to 2 dimensions)\n",
        "n_components_pca = 2\n",
        "X_pca, W_pca, eigen_pairs_pca = pca(X, n_components_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v92waiVAd2fU"
      },
      "source": [
        "PCA visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JGDL-hSd2fU"
      },
      "outputs": [],
      "source": [
        "def plot_reduced_data(X_reduced, y, title):\n",
        "    \"\"\"Visualize the 2D reduced data\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i, target_name in enumerate(target_names):\n",
        "        plt.scatter(X_reduced[y == i, 0], X_reduced[y == i, 1], label=target_name)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "print(\"--- PCA Projection ---\")\n",
        "plot_reduced_data(X_pca, y, 'PCA Projection of Iris Dataset (2 Components)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDJt7JA3B0Sc"
      },
      "source": [
        "Part 2: Linear Discriminant Analysis (LDA)\n",
        "\n",
        "LDA is a **supervised** method that aims to find the directions that maximize the ratio of **Between-Class Scatter (SB)** to **Within-Class Scatter (SW)** for optimal class separation.\n",
        "\n",
        "## Implementation Steps:\n",
        "1. Calculate SW and SB.\n",
        "2. Solve the generalized eigenvalue problem SB(SW^-1 * SB).\n",
        "3. Select the top c-1 (2 for Iris) discriminant components and project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTVfI3l4mWIF"
      },
      "outputs": [],
      "source": [
        "def calculate_scatter_matrices(X, y):\n",
        "    \"\"\"Calculate S_W and S_B\"\"\"\n",
        "\n",
        "    n_features = X.shape[1]\n",
        "    unique_classes = np.unique(y)\n",
        "\n",
        "    mean_overall = np.mean(X, axis=0)\n",
        "\n",
        "    # S_W\n",
        "    S_W = np.zeros((n_features, n_features))\n",
        "    for i in unique_classes:\n",
        "        X_class = X[y == i]\n",
        "        # TODO: Fill in the logic for S_i or the sum of within-class covariances\n",
        "        scatter_i = # 【BLANK 4: Calculate S_i or within-class covariance sum】\n",
        "        S_W += scatter_i\n",
        "\n",
        "    # S_B\n",
        "    S_B = np.zeros((n_features, n_features))\n",
        "    for i in unique_classes:\n",
        "        X_class = X[y == i]\n",
        "        n_i = X_class.shape[0]\n",
        "        mean_class = np.mean(X_class, axis=0)\n",
        "        mean_diff = (mean_class - mean_overall).reshape(n_features, 1)\n",
        "\n",
        "        # TODO: Fill in the logic for n_i * (mean_diff * mean_diff.T)\n",
        "        S_B += # 【BLANK 5: Calculate n_i * (mean_diff * mean_diff.T)】\n",
        "\n",
        "    return S_W, S_B\n",
        "\n",
        "def lda(X, y, n_components):\n",
        "    \"\"\"Execute LDA\"\"\"\n",
        "\n",
        "    S_W, S_B = calculate_scatter_matrices(X, y)\n",
        "\n",
        "    # Tikhonov Regularization to handle S_W singularity\n",
        "    epsilon = 1e-6\n",
        "    S_W_reg = S_W + epsilon * np.identity(S_W.shape[0])\n",
        "\n",
        "    # Solve S_W^-1 * S_B\n",
        "    S_W_inv = inv(S_W_reg)\n",
        "    M = np.dot(S_W_inv, S_B)\n",
        "\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(M)\n",
        "    eigenvalues = eigenvalues.real\n",
        "    eigenvectors = eigenvectors.real\n",
        "\n",
        "    eigen_pairs = [(eigenvalues[i], eigenvectors[:, i]) for i in range(len(eigenvalues))]\n",
        "\n",
        "    # Sort: descending order\n",
        "    # TODO: Fill in the key for sorting Eigenpairs\n",
        "    eigen_pairs.sort(key= # 【BLANK 6: Eigenvalue sorting key】 , reverse=True)\n",
        "\n",
        "    W = np.array([eigen_pairs[i][1] for i in range(n_components)]).T\n",
        "\n",
        "    # Project\n",
        "    # TODO: Fill in the projection formula\n",
        "    X_lda = # 【BLANK 7: Projection formula】\n",
        "\n",
        "    return X_lda, W, eigen_pairs\n",
        "\n",
        "# Execute LDA (Reduce to 2 dimensions)\n",
        "n_components_lda = 2\n",
        "X_lda, W_lda, eigen_pairs_lda = lda(X, y, n_components_lda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvs4QxaMB5GO"
      },
      "source": [
        "LDA Visualization and Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI-r0BazmXur"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- LDA Projection ---\")\n",
        "plot_reduced_data(X_lda, y, 'LDA Projection of Iris Dataset (2 Components)')\n",
        "\n",
        "# Output summary data for the report\n",
        "print(\"\\n--- Summary Data (for Report) ---\")\n",
        "# Assuming you captured eigen_pairs_pca earlier\n",
        "print(\"PCA Eigenvalues (Variance):\", [pair[0] for pair in eigen_pairs_pca][:2])\n",
        "print(\"LDA Eigenvalues (Between/Within Ratio):\", [pair[0] for pair in eigen_pairs_lda][:2])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}